## Filebeat configuration
## https://github.com/elastic/beats/blob/master/deploy/docker/filebeat.docker.yml
#
## Ejemplo completo de filebeat.yml
## https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html
#

filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

filebeat.autodiscover:
  providers:
    # The Docker autodiscover provider automatically retrieves logs from Docker
    # containers as they start and stop.
    - type: docker
      hints.enabled: true

#================================ Filebeat inputs ====================================
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /tmp/rpm-*.log
  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
  # No fuciona con expresiones grook --> ^%{TIMESTAMP_ISO8601}
  multiline.type: pattern
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
  # Defines if the pattern set under pattern should be negated or not. Default is false.
  multiline.negate: true
  # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
  multiline.match: after
  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  # Esto consume batante recurso, otra alternativa es que logstash lo quite.
  exclude_lines: ['^[0-9]{4}-[0-9]{2}-[0-9]{2}.*DEBUG']
  tags: ["worker_logs"]

- type: log
  enabled: true
  paths:
    - /ms-process-manager/ms_process_manager.log
  multiline.type: pattern
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
  multiline.negate: true
  multiline.match: after
  exclude_lines: ['^[0-9]{4}-[0-9]{2}-[0-9]{2}.*DEBUG']
  tags: ["ms_manager_logs"]

# ================================= Processors =================================
# Processors are used to reduce the number of fields in the exported event or to
# enhance the event with external metadata. This section defines a list of
# processors that are applied one by one and the first one receives the initial
#
# The supported processors are drop_fields, drop_event, include_fields,
# decode_json_fields, and add_cloud_metadata.
#
processors:
- drop_fields:
    fields: ["agent", "input", "host", "ecs"]
# - add_locale:
#     format: offset

#================================ Outputs console =====================================
# Configure what output to use when sending the data collected by the beat.
# output.console:
#   pretty: true

#================================ Elasticsearch output ================================
# output.elasticsearch:
#   hosts: ['http://elasticsearch:9200']
#   username: elastic
#   password: ${ELASTIC_PASSWORD}

## HTTP endpoint for health checking
## https://www.elastic.co/guide/en/beats/filebeat/current/http-endpoint.html

#================================== Logstash output ====================================
output.logstash:
  # The Logstash hosts
  hosts: ["10.20.128.201:5044"]
  ttl: 120
  timeout: 60
  #password: ${LOGSTASH_INTERNAL_PASSWORD:-}

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"


http.enabled: true
http.host: 0.0.0.0
